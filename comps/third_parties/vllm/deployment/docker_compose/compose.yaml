# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  vllm-server:
    image: ${REGISTRY:-opea}/vllm:${TAG:-latest}
    container_name: vllm-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:80
    volumes:
      - "${DATA_PATH:-./data}:/data"
    shm_size: 128g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_TORCH_PROFILER_DIR: "${VLLM_TORCH_PROFILER_DIR:-/mnt}"
      host_ip: ${host_ip}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      VLLM_SKIP_WARMUP: ${VLLM_SKIP_WARMUP:-false}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
    command: --model $LLM_MODEL_ID --host 0.0.0.0 --port 80
  vllm-gaudi-server:
    image: ${REGISTRY:-opea}/vllm-gaudi:${TAG:-latest}
    container_name: vllm-gaudi-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:80
    volumes:
      - "${DATA_PATH:-./data}:/data"
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_TORCH_PROFILER_DIR: "/mnt"
      host_ip: ${host_ip}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      VLLM_SKIP_WARMUP: ${VLLM_SKIP_WARMUP:-false}
      MAX_MODEL_LEN: ${MAX_TOTAL_TOKENS:-4096}
      MAX_SEQ_LEN_TO_CAPTURE: ${MAX_TOTAL_TOKENS:-4096}
    runtime: habana
    cap_add:
      - SYS_NICE
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
    command: --model $LLM_MODEL_ID --tensor-parallel-size 1 --host 0.0.0.0 --port 80 --block-size 128 --max-num-seqs 256
  vllm-openvino:
    image: ${REGISTRY:-opea}/vllm-openvino:${TAG:-latest}
    container_name: vllm-openvino
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:${LLM_ENDPOINT_PORT:-8008}
    volumes:
      - "${HF_CACHE_DIR:-$HOME/.cache/huggingface}:/root/.cache/huggingface"
    environment:
      HTTPS_PROXY: ${http_proxy}
      HTTP_PROXY: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      host_ip: ${host_ip}
    entrypoint: /bin/bash -c " cd / && export VLLM_CPU_KVCACHE_SPACE=50 && python3 -m vllm.entrypoints.openai.api_server   --model ${LLM_MODEL_ID}   --host 0.0.0.0  --port ${LLM_ENDPOINT_PORT}"
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
  vllm-openvino-arc:
    image: ${REGISTRY:-opea}/vllm-arc:${TAG:-latest}
    container_name: vllm-openvino-arc
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:${LLM_ENDPOINT_PORT:-8008}
    volumes:
      - "${HF_CACHE_DIR:-$HOME/.cache/huggingface}:/root/.cache/huggingface"
    devices:
      - "/dev/dri:/dev/dri"
    group_add:
      - ${RENDER_GROUP_ID:-110}
    environment:
      HTTPS_PROXY: ${http_proxy}
      HTTP_PROXY: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      host_ip: ${host_ip}
    entrypoint: /bin/bash -c " export VLLM_OPENVINO_DEVICE=GPU &&  export VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=ON &&  python3 -m vllm.entrypoints.openai.api_server    --model ${LLM_MODEL_ID}    --host 0.0.0.0    --port ${LLM_ENDPOINT_PORT}    --max_model_len 8192"
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100

networks:
  default:
    driver: bridge
