# Path to all videos
videos: video_ingest/videos/
# Do you want to extract frames of videos (True if not done already, else False)
generate_frames: True
# How do you want to generate feature embeddings?
embeddings:
  type: 'video' 
  vclip_model_name: "openai/clip-vit-base-patch32"
  vclip_num_frm: 64
  path: 'video_ingest/embeddings'
# VL-branch config
vl_branch:
  cfg_path: embedding/video_llama_config/video_llama_eval_only_vl.yaml
  model_type: 'llama_v2'
# Path to store metadata files
meta_output_dir: video_ingest/video_metadata/
# Chunk duration defines the interval of time that each embedding will occur
chunk_duration: 30
# Clip duration defines the length of the interval in which the embeding will occur
clip_duration: 10
# e.g. For every <chunk_duration>, you embed the first <clip_duration>'s frames of that interval

vector_db:
  choice_of_db: 'vdms' # #Supported databases [vdms]
  host: 0.0.0.0
  port: 55555 


# LLM path
model_path: meta-llama/Llama-2-7b-chat-hf