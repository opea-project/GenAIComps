# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Path to all videos
videos: uploaded_files/
# Do you want to extract frames of videos (True if not done already, else False)
generate_frames: True
# How do you want to generate feature embeddings?
embeddings:
  vclip_model_name: "openai/clip-vit-base-patch32"
  vclip_num_frm: 64
  vector_dimensions: 512
  path: "uploaded_files/embeddings"
# VL-branch config
vl_branch:
  cfg_path: embedding/video_llama_config/video_llama_eval_only_vl.yaml
  model_type: "llama_v2"
# Path to store metadata files
meta_output_dir: uploaded_files/video_metadata/
# Chunk duration defines the interval of time that each embedding will occur
chunk_duration: 30
# Clip duration defines the length of the interval in which the embedding will occur
clip_duration: 10
# e.g. For every <chunk_duration>, you embed the first <clip_duration>'s frames of that interval

vector_db:
  choice_of_db: "vdms" # #Supported databases [vdms]

# LLM path
model_path: meta-llama/Llama-2-7b-chat-hf
