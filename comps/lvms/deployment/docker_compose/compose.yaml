# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

include:
  - ../../../third_parties/llama-vision/deployment/docker_compose/compose.yaml
  - ../../../third_parties/llava/deployment/docker_compose/compose.yaml
  - ../../../third_parties/predictionguard/deployment/docker_compose/compose.yaml
  - ../../../third_parties/video-llama/deployment/docker_compose/compose.yaml

services:
  vllm-service:
    image: ${REGISTRY:-opea}/vllm:latest
    container_name: vllm-service
    ports:
      - ${VLLM_PORT:-9699}:80
    volumes:
      - "./data:/root/.cache/huggingface/hub/"
    shm_size: 128g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_TORCH_PROFILER_DIR: "/mnt"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
    command: --model $LLM_MODEL_ID --host 0.0.0.0 --port 80 # --chat-template examples/template_llava.jinja  # https://docs.vllm.ai/en/v0.5.0/models/vlm.html
  vllm-gaudi-service:
    image: ${REGISTRY:-opea}/vllm-gaudi:${TAG:-latest}
    container_name: vllm-gaudi-service
    ports:
      - ${VLLM_PORT:-9699}:80
    volumes:
      - "./data:/root/.cache/huggingface/hub/"
    shm_size: 128g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_TORCH_PROFILER_DIR: "/mnt"
      VLLM_SKIP_WARMUP: ${VLLM_SKIP_WARMUP:-false}
      MAX_MODEL_LEN: ${MAX_TOTAL_TOKENS:-4096}
      MAX_SEQ_LEN_TO_CAPTURE: ${MAX_TOTAL_TOKENS:-4096}
      PT_HPUGRAPH_DISABLE_TENSOR_CACHE: false # https://github.com/HabanaAI/vllm-fork/issues/841#issuecomment-2700421704
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true # for tensor parallel inference with hpu graphs
    runtime: habana
    cap_add:
      - SYS_NICE
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 150
    command: --model $LLM_MODEL_ID --tensor-parallel-size ${TP_SIZE:-1} --host 0.0.0.0 --port 80 --enable-auto-tool-choice --tool-call-parser hermes  --chat-template ${CHAT_TEMPLATE:-examples/template_llava.jinja} # https://docs.vllm.ai/en/v0.5.0/models/vlm.html
  llava-tgi-service:
    image: ghcr.io/huggingface/tgi-gaudi:2.3.1
    container_name: llava-tgi-service
    ports:
      - ${LLAVA_TGI_PORT:-5027}:80
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
    runtime: habana
    cap_add:
      - SYS_NICE
    ipc: host
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 6s
      retries: 20
    command: --model-id ${LLM_MODEL_ID} --max-input-length 4096 --max-total-tokens 8192
  lvm:
    image: ${REGISTRY:-opea}/lvm:${TAG:-latest}
    container_name: lvm-service
    ports:
      - ${LVM_PORT:-9399}:9399
    ipc: host
    environment:
      LVM_ENDPOINT: ${LVM_ENDPOINT}
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_VLLM_LVM}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      LOGFLAG: ${LOGFLAG}
      ENABLE_MCP: ${ENABLE_MCP:-False}
  lvm-llava:
    extends: lvm
    container_name: lvm-llava-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_LLAVA_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      llava-service:
        condition: service_healthy
  lvm-llava-tgi:
    extends: lvm
    container_name: lvm-llava-tgi-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_TGI_LLAVA_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      llava-tgi-service:
        condition: service_healthy
  lvm-llama-vision:
    extends: lvm
    container_name: lvm-llama-vision-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_LLAMA_VISION_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      llama-vision-service:
        condition: service_healthy
  lvm-predictionguard:
    extends: lvm
    container_name: lvm-predictionguard-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_PREDICTION_GUARD_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      predictionguard-service:
        condition: service_healthy
  lvm-video-llama:
    extends: lvm
    container_name: lvm-video-llama-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_VIDEO_LLAMA_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      video-llama-service:
        condition: service_healthy
  lvm-vllm:
    extends: lvm
    container_name: lvm-vllm-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_VLLM_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      vllm-service:
        condition: service_healthy
  lvm-vllm-gaudi:
    extends: lvm
    container_name: lvm-vllm-gaudi-service
    environment:
      LVM_COMPONENT_NAME: ${LVM_COMPONENT_NAME:-OPEA_VLLM_LVM}
      ENABLE_MCP: ${ENABLE_MCP:-False}
    depends_on:
      vllm-gaudi-service:
        condition: service_healthy
  lvm-vllm-ipex-service:
    container_name: lvm-vllm-ipex-service
    image: ${REGISTRY:-intel}/llm-scaler-vllm:${TAG:-latest}
    privileged: true
    restart: always
    ports:
      - ${VLLM_PORT:-41091}:8000
    group_add:
      - ${VIDEO_GROUP_ID:-44}           # Use the environment variable for the video group ID
      - ${RENDER_GROUP_ID:-992}         # Use the environment variable for the render group ID
    volumes:
      - ${HF_HOME:-${HOME}/.cache/huggingface}:/llm/.cache/huggingface
      - ../../src/vllm_ipex_entrypoint.sh:/llm/vllm_ipex_entrypoint.sh
    devices:
      - /dev/dri
    environment:
      no_proxy: localhost,127.0.0.1
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_ENDPOINT: https://hf-mirror.com
      HF_HOME: /llm/.cache/huggingface
      MODEL_PATH: ${LLM_MODEL_ID}
      SERVED_MODEL_NAME: ${LLM_MODEL_ID}
      TENSOR_PARALLEL_SIZE: ${TENSOR_PARALLEL_SIZE:-1}
      MAX_MODEL_LEN: ${MAX_MODEL_LEN:-20000}
      ONEAPI_DEVICE_SELECTOR: ${ONEAPI_DEVICE_SELECTOR:-level_zero:0}
      LOAD_QUANTIZATION: ${LOAD_QUANTIZATION:-fp8}
      ZE_AFFINITY_MASK: ${ZE_AFFINITY_MASK}
    shm_size: 128g
    entrypoint: /bin/bash -c "\
      source /opt/intel/oneapi/setvars.sh --force && \
      bash /llm/vllm_ipex_entrypoint.sh"

networks:
  default:
    driver: bridge
