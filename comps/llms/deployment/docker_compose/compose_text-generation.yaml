# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

include:
  - ../../../third_parties/tgi/deployment/docker_compose/compose.yaml
  - ../../../third_parties/vllm/deployment/docker_compose/compose.yaml
  - ../../../third_parties/ollama/deployment/docker_compose/compose.yaml
  - ../../../third_parties/ovms/deployment/docker_compose/compose.yaml

services:
  textgen:
    image: ${REGISTRY:-opea}/llm-textgen:${TAG:-latest}
    container_name: llm-textgen-server
    ports:
      - ${TEXTGEN_PORT:-9000}:9000
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      LLM_ENDPOINT: ${LLM_ENDPOINT}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      HF_TOKEN: ${HF_TOKEN}
      LOGFLAG: ${LOGFLAG:-False}
    restart: unless-stopped

  textgen-gaudi:
    image: ${REGISTRY:-opea}/llm-textgen-gaudi:${TAG:-latest}
    container_name: llm-textgen-gaudi-server
    ports:
      - ${TEXTGEN_PORT:-9000}:9000
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      LLM_ENDPOINT: ${LLM_ENDPOINT}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      HF_TOKEN: ${HF_TOKEN}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      TOKENIZERS_PARALLELISM: False
      LOGFLAG: ${LOGFLAG:-False}
    runtime: habana
    cap_add:
      - SYS_NICE
    restart: unless-stopped

  textgen-phi4-gaudi:
    image: ${REGISTRY:-opea}/llm-textgen-phi4-gaudi:${TAG:-latest}
    container_name: llm-textgen-phi4-gaudi-server
    ports:
      - ${TEXTGEN_PORT:-9000}:9000
    volumes:
      - "${DATA_PATH:-./data}:/data"
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      HF_TOKEN: ${HF_TOKEN}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      TOKENIZERS_PARALLELISM: False
      LOGFLAG: ${LOGFLAG:-False}
    runtime: habana
    cap_add:
      - SYS_NICE
    restart: unless-stopped

  textgen-service-tgi:
    extends: textgen
    container_name: textgen-service-tgi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}
    depends_on:
      tgi-server:
        condition: service_healthy

  textgen-service-tgi-gaudi:
    extends: textgen
    container_name: textgen-service-tgi-gaudi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}
    depends_on:
      tgi-gaudi-server:
        condition: service_healthy

  textgen-service-vllm:
    extends: textgen
    container_name: textgen-service-vllm
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}
    depends_on:
      vllm-server:
        condition: service_healthy

  textgen-service-vllm-gaudi:
    extends: textgen
    container_name: textgen-service-vllm-gaudi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}
    depends_on:
      vllm-gaudi-server:
        condition: service_healthy

  textgen-service-ollama:
    extends: textgen
    container_name: textgen-service-ollama
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}

  textgen-predictionguard:
    extends: textgen
    container_name: textgen-predictionguard
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenPredictionguard}
      PREDICTIONGUARD_API_KEY: ${PREDICTIONGUARD_API_KEY}

  textgen-service-endpoint-openai:
    extends: textgen
    container_name: textgen-service-endpoint-openai
    environment:
      LLM_COMPONENT_NAME: OpeaTextGenService
      LLM_ENDPOINT: ${LLM_ENDPOINT} # an endpoint that uses OpenAI API style e.g., https://openrouter.ai/api
      OPENAI_API_KEY: ${OPENAI_API_KEY} # the key associated with the endpoint
      LLM_MODEL_ID: ${LLM_MODEL_ID:-google/gemma-3-1b-it:free}

  textgen-native-gaudi:
    extends: textgen-gaudi
    container_name: textgen-native-gaudi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenNative}

  textgen-native-phi4-gaudi:
    extends: textgen-phi4-gaudi
    container_name: textgen-native-phi4-gaudi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenNative}

  textgen-native-phi4-multimodal-gaudi:
    extends: textgen-phi4-gaudi
    container_name: textgen-native-phi4-multimodal-gaudi
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenNativePhi4Multimodal}

  textgen-service-ovms:
    extends: textgen
    container_name: textgen-service-ovms
    environment:
      LLM_COMPONENT_NAME: ${LLM_COMPONENT_NAME:-OpeaTextGenService}
      OVMS_LLM_ENDPOINT: ${OVMS_LLM_ENDPOINT}
      MODEL_ID: ${MODEL_ID}
    depends_on:
      ovms-llm-serving:
        condition: service_healthy

  textgen-vllm-ipex-service:
    container_name: textgen-vllm-ipex-service
    image: ${REGISTRY:-intel}/llm-scaler-vllm:${TAG:-latest}
    privileged: true
    restart: always
    ports:
      - ${VLLM_PORT:-41090}:8000
    group_add:
      - ${VIDEO_GROUP_ID:-44}           # Use the environment variable for the video group ID
      - ${RENDER_GROUP_ID:-992}         # Use the environment variable for the render group ID
    volumes:
      - ${HF_HOME:-${HOME}/.cache/huggingface}:/llm/.cache/huggingface
      - ../../src/text-generation/vllm_ipex_entrypoint.sh:/llm/vllm_ipex_entrypoint.sh
    devices:
      - /dev/dri
    environment:
      no_proxy: localhost,127.0.0.1
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_ENDPOINT: https://hf-mirror.com
      HF_HOME: /llm/.cache/huggingface
      MODEL_PATH: ${LLM_MODEL_ID}
      SERVED_MODEL_NAME: ${LLM_MODEL_ID}
      TENSOR_PARALLEL_SIZE: ${TENSOR_PARALLEL_SIZE:-1}
      MAX_MODEL_LEN: ${MAX_MODEL_LEN:-20000}
      ONEAPI_DEVICE_SELECTOR: ${ONEAPI_DEVICE_SELECTOR:-level_zero:0}
      LOAD_QUANTIZATION: ${LOAD_QUANTIZATION:-fp8}
      ZE_AFFINITY_MASK: ${ZE_AFFINITY_MASK}
    shm_size: 128g
    entrypoint: /bin/bash -c "\
      source /opt/intel/oneapi/setvars.sh --force && \
      bash /llm/vllm_ipex_entrypoint.sh"

networks:
  default:
    driver: bridge
