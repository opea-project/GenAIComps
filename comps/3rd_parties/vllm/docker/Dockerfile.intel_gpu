# The vLLM Dockerfile is used to construct vLLM image that can be directly used
# to run the OpenAI compatible server.
# Based on https://github.com/vllm-project/vllm/blob/main/Dockerfile.openvino
# add Intel ARC support package

FROM ubuntu:22.04 AS dev

RUN apt-get update -y && \
    apt-get install -y \
        git python3-pip \
        ffmpeg libsm6 libxext6 libgl1 \
        gpg-agent wget

RUN wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | gpg --yes --dearmor --output /usr/share/keyrings/intel-graphics.gpg && \
    echo "deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy/lts/2350 unified" | \
        tee /etc/apt/sources.list.d/intel-gpu-jammy.list &&\
    apt update -y &&\
    apt install -y \
        intel-opencl-icd intel-level-zero-gpu level-zero \
        intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \
        libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \
        libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \
        mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo hwinfo clinfo

WORKDIR /workspace

RUN git clone -b v0.6.3.post1 https://github.com/vllm-project/vllm.git

#ARG GIT_REPO_CHECK=0
#RUN --mount=type=bind,source=.git,target=.git \
#    if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh ; fi

# install build requirements
RUN PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu" python3 -m pip install -r /workspace/vllm/requirements-build.txt
# build vLLM with OpenVINO backend
RUN PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu" VLLM_TARGET_DEVICE="openvino" python3 -m pip install /workspace/vllm/

#COPY examples/ /workspace/vllm/examples
#COPY benchmarks/ /workspace/vllm/benchmarks


CMD ["/bin/bash"]

