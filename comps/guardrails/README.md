# Guardrails Microservices

The Guardrails microservices offers two primary types of guardrails:

- Input Guardrails: These are applied to user inputs. An input guardrail can either reject the input, halting further processing.
- Output Guardrails: These are applied to outputs generated by the LLM. An output guardrail can reject the output, preventing it from being returned to the user.

The Guardrails components provide various microservices providing the following kinds of GenAI safeguards:
- [factual consistency](factuality) - For validating the accuracy of model outputs and preventing hallucinations
- [PII detection](pii_detection) - For filtering out sensitive data in model inputs or outputs
- [Prompt injection detection](prompt_injection) - For blocking malicious prompts to models
- [Toxicity and harm](toxicity_harm) - For filtering toxic model inputs or outputs

Follow the links to the respective directories for information about deploying the microservices.
