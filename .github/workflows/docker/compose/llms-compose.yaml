# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# this file should be run in the root of the repo
services:
  llm-tgi:
    build:
      dockerfile: comps/llms/text-generation/tgi/Dockerfile
    image: ${REGISTRY:-opea}/llm-tgi:${TAG:-latest}
  llm-ollama:
    build:
      dockerfile: comps/llms/text-generation/ollama/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-ollama:${TAG:-latest}
  llm-docsum-tgi:
    build:
      dockerfile: comps/llms/summarization/tgi/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-docsum-tgi:${TAG:-latest}
  llm-faqgen-tgi:
    build:
      dockerfile: comps/llms/faq-generation/tgi/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-faqgen-tgi:${TAG:-latest}
  llm-vllm:
    build:
      dockerfile: comps/llms/text-generation/vllm/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-vllm:${TAG:-latest}
  llm-native:
    build:
      dockerfile: comps/llms/text-generation/native/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-native:${TAG:-latest}
  llm-native-llamaindex:
    build:
      dockerfile: comps/llms/text-generation/native/llama_index/Dockerfile
    image: ${REGISTRY:-opea}/llm-native-llamaindex:${TAG:-latest}
  vllm-openvino:
    build:
      context: vllm-openvino
      dockerfile: Dockerfile.openvino
    image: ${REGISTRY:-opea}/vllm-openvino:${TAG:-latest}
  vllm-gaudi:
    build:
      context: vllm-fork
      dockerfile: Dockerfile.hpu
    shm_size: '128g'
    image: ${REGISTRY:-opea}/vllm-gaudi:${TAG:-latest}
  vllm-arc:
    build:
      dockerfile: comps/llms/text-generation/vllm/langchain/dependency/Dockerfile.intel_gpu
    image: ${REGISTRY:-opea}/vllm-arc:${TAG:-latest}
  llm-eval:
    build:
      dockerfile: comps/llms/utils/lm-eval/Dockerfile
    image: ${REGISTRY:-opea}/llm-eval:${TAG:-latest}
  llm-vllm-llamaindex:
    build:
      dockerfile: comps/llms/text-generation/vllm/llama_index/Dockerfile
    image: ${REGISTRY:-opea}/llm-vllm-llamaindex:${TAG:-latest}
  llm-textgen-predictionguard:
    build:
      dockerfile: comps/llms/text-generation/predictionguard/Dockerfile
    image: ${REGISTRY:-opea}/llm-textgen-predictionguard:${TAG:-latest}
  llm-docsum-vllm:
    build:
      dockerfile: comps/llms/summarization/vllm/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-docsum-vllm:${TAG:-latest}
  llm-faqgen-vllm:
    build:
      dockerfile: comps/llms/faq-generation/vllm/langchain/Dockerfile
    image: ${REGISTRY:-opea}/llm-faqgen-vllm:${TAG:-latest}
