INFO 12-09 23:52:00 api_server.py:625] vLLM API server version 0.6.3.dev1405+gdb686905
INFO 12-09 23:52:00 api_server.py:626] args: Namespace(host='0.0.0.0', port=80, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-70B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', weights_load_device=None, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=128, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, use_padding_aware_scheduling=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_num_prefill_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=16384, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 12-09 23:52:00 __init__.py:60] No plugins found.
INFO 12-09 23:52:00 api_server.py:178] Multiprocessing frontend to use ipc:///tmp/b03e0fed-aee3-48aa-ab57-8675627b48e2 for IPC Path.
INFO 12-09 23:52:00 api_server.py:197] Started engine process with PID 77
INFO 12-09 23:52:04 __init__.py:60] No plugins found.
INFO 12-09 23:52:07 config.py:403] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 12-09 23:52:07 config.py:1042] Defaulting to use mp for distributed inference
WARNING 12-09 23:52:07 arg_utils.py:1104] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
WARNING 12-09 23:52:07 arg_utils.py:1160] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-09 23:52:12 config.py:403] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-09 23:52:12 config.py:1042] Defaulting to use mp for distributed inference
WARNING 12-09 23:52:12 arg_utils.py:1104] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
WARNING 12-09 23:52:12 arg_utils.py:1160] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
INFO 12-09 23:52:12 llm_engine.py:250] Initializing an LLM engine (v0.6.3.dev1405+gdb686905) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, weights_load_device=hpu, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=hpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None, pooler_config=None,compilation_config=CompilationConfig(level=0, backend='', custom_ops=[], splitting_ops=['vllm.unified_attention', 'vllm.unified_attention_with_output'], use_inductor=True, inductor_specialize_for_cudagraph_no_more_than=None, inductor_compile_sizes=[], inductor_compile_config={}, inductor_passes={}, use_cudagraph=False, cudagraph_num_of_warmups=0, cudagraph_capture_sizes=None, cudagraph_copy_inputs=False, pass_config=PassConfig(dump_graph_stages=[], dump_graph_dir=PosixPath('.'), enable_fusion=True, enable_reshape=True), compile_sizes=[], capture_sizes=[256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], enabled_custom_ops=Counter(), disabled_custom_ops=Counter(), compilation_time=0.0, static_forward_context={})
WARNING 12-09 23:52:13 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-09 23:52:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
Detected capabilities: [-cpu -gaudi +gaudi2 -gaudi3 -index_reduce]
WARNING 12-09 23:52:16 utils.py:772] Pin memory is not supported on HPU.
INFO 12-09 23:52:16 selector.py:159] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=358)[0;0m Detected capabilities: [-cpu -gaudi +gaudi2 -gaudi3 -index_reduce]
[1;36m(VllmWorkerProcess pid=358)[0;0m WARNING 12-09 23:52:16 utils.py:772] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:16 selector.py:159] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=357)[0;0m Detected capabilities: [-cpu -gaudi +gaudi2 -gaudi3 -index_reduce]
[1;36m(VllmWorkerProcess pid=357)[0;0m WARNING 12-09 23:52:16 utils.py:772] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:16 selector.py:159] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_SEQ_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_SEQ_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_PROMPT_SEQ_BUCKET_MAX=1024 (default:1024)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BLOCK_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=358)[0;0m VLLM_DECODE_BLOCK_BUCKET_MAX=4096 (default:4096)
[1;36m(VllmWorkerProcess pid=358)[0;0m Prompt bucket config (min, step, max_warmup) bs:[1, 32, 256], seq:[128, 128, 1024]
[1;36m(VllmWorkerProcess pid=358)[0;0m Decode bucket config (min, step, max_warmup) bs:[1, 32, 256], block:[128, 128, 4096]
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_SEQ_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_SEQ_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_PROMPT_SEQ_BUCKET_MAX=1024 (default:1024)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BLOCK_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=357)[0;0m VLLM_DECODE_BLOCK_BUCKET_MAX=4096 (default:4096)
[1;36m(VllmWorkerProcess pid=357)[0;0m Prompt bucket config (min, step, max_warmup) bs:[1, 32, 256], seq:[128, 128, 1024]
[1;36m(VllmWorkerProcess pid=357)[0;0m Decode bucket config (min, step, max_warmup) bs:[1, 32, 256], block:[128, 128, 4096]
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=359)[0;0m Detected capabilities: [-cpu -gaudi +gaudi2 -gaudi3 -index_reduce]
[1;36m(VllmWorkerProcess pid=359)[0;0m WARNING 12-09 23:52:16 utils.py:772] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:16 selector.py:159] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BS_BUCKET_MIN=1 (default:1)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BS_BUCKET_STEP=32 (default:32)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BS_BUCKET_MAX=256 (default:256)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_SEQ_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_SEQ_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_PROMPT_SEQ_BUCKET_MAX=1024 (default:1024)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BLOCK_BUCKET_MIN=128 (default:128)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:128)
[1;36m(VllmWorkerProcess pid=359)[0;0m VLLM_DECODE_BLOCK_BUCKET_MAX=4096 (default:4096)
[1;36m(VllmWorkerProcess pid=359)[0;0m Prompt bucket config (min, step, max_warmup) bs:[1, 32, 256], seq:[128, 128, 1024]
[1;36m(VllmWorkerProcess pid=359)[0;0m Decode bucket config (min, step, max_warmup) bs:[1, 32, 256], block:[128, 128, 4096]
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374408 KB
------------------------------------------------------------------------------
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374408 KB
------------------------------------------------------------------------------
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374408 KB
------------------------------------------------------------------------------
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374408 KB
------------------------------------------------------------------------------
VLLM_PROMPT_BS_BUCKET_MIN=1 (default:1)
VLLM_PROMPT_BS_BUCKET_STEP=32 (default:32)
VLLM_PROMPT_BS_BUCKET_MAX=256 (default:256)
VLLM_DECODE_BS_BUCKET_MIN=1 (default:1)
VLLM_DECODE_BS_BUCKET_STEP=32 (default:32)
VLLM_DECODE_BS_BUCKET_MAX=256 (default:256)
VLLM_PROMPT_SEQ_BUCKET_MIN=128 (default:128)
VLLM_PROMPT_SEQ_BUCKET_STEP=128 (default:128)
VLLM_PROMPT_SEQ_BUCKET_MAX=1024 (default:1024)
VLLM_DECODE_BLOCK_BUCKET_MIN=128 (default:128)
VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:128)
VLLM_DECODE_BLOCK_BUCKET_MAX=4096 (default:4096)
Prompt bucket config (min, step, max_warmup) bs:[1, 32, 256], seq:[128, 128, 1024]
Decode bucket config (min, step, max_warmup) bs:[1, 32, 256], block:[128, 128, 4096]
INFO 12-09 23:52:21 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f54d9e04fd0>, local_subscribe_port=42205, remote_subscribe_port=None)
INFO 12-09 23:52:23 loader.py:368] Loading weights on hpu...
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:23 loader.py:368] Loading weights on hpu...
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:23 loader.py:368] Loading weights on hpu...
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:23 loader.py:368] Loading weights on hpu...
INFO 12-09 23:52:23 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:23 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:23 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:23 weight_utils.py:243] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:01<00:46,  1.60s/it]

Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:02<00:35,  1.28s/it]

Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:03<00:34,  1.29s/it]

Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:06<00:47,  1.83s/it]

Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:09<00:51,  2.04s/it]

Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:10<00:46,  1.95s/it]

Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:13<00:52,  2.28s/it]

Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:15<00:49,  2.26s/it]

Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:16<00:37,  1.79s/it]

Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:17<00:29,  1.47s/it]

Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:18<00:23,  1.26s/it]

Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:19<00:23,  1.28s/it]

Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:20<00:22,  1.30s/it]

Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:22<00:20,  1.26s/it]

Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:22<00:15,  1.05s/it]

Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:23<00:12,  1.11it/s]

Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:23<00:10,  1.24it/s]

Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:24<00:09,  1.30it/s]

Loading safetensors checkpoint shards:  63% Completed | 19/30 [00:25<00:07,  1.40it/s]

Loading safetensors checkpoint shards:  67% Completed | 20/30 [00:25<00:06,  1.43it/s]

Loading safetensors checkpoint shards:  70% Completed | 21/30 [00:26<00:05,  1.57it/s]

Loading safetensors checkpoint shards:  73% Completed | 22/30 [00:26<00:05,  1.57it/s]

Loading safetensors checkpoint shards:  77% Completed | 23/30 [00:27<00:04,  1.57it/s]

Loading safetensors checkpoint shards:  80% Completed | 24/30 [00:28<00:03,  1.54it/s]

Loading safetensors checkpoint shards:  83% Completed | 25/30 [00:28<00:02,  1.69it/s]

Loading safetensors checkpoint shards:  87% Completed | 26/30 [00:29<00:02,  1.66it/s]

Loading safetensors checkpoint shards:  90% Completed | 27/30 [00:29<00:01,  1.73it/s]

Loading safetensors checkpoint shards:  93% Completed | 28/30 [00:30<00:01,  1.82it/s]

Loading safetensors checkpoint shards:  97% Completed | 29/30 [00:30<00:00,  1.91it/s]

Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:31<00:00,  1.92it/s]

Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:31<00:00,  1.04s/it]

INFO 12-09 23:52:55 hpu_model_runner.py:668] Pre-loading model weights on hpu:0 took 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.121 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:55 hpu_model_runner.py:668] Pre-loading model weights on hpu:0 took 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.112 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:55 hpu_model_runner.py:668] Pre-loading model weights on hpu:0 took 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.118 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:55 hpu_model_runner.py:668] Pre-loading model weights on hpu:0 took 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.117 GiB of host memory (127.2 GiB/1007 GiB used)
INFO 12-09 23:52:56 hpu_model_runner.py:742] Wrapping in HPU Graph took 0 B of device memory (32.95 GiB/94.62 GiB used) and -40 KiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:742] Wrapping in HPU Graph took 0 B of device memory (32.95 GiB/94.62 GiB used) and -40 KiB of host memory (127.2 GiB/1007 GiB used)
INFO 12-09 23:52:56 hpu_model_runner.py:746] Loading model weights took in total 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.125 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:746] Loading model weights took in total 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.118 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:742] Wrapping in HPU Graph took 0 B of device memory (32.95 GiB/94.62 GiB used) and -32 KiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:742] Wrapping in HPU Graph took 0 B of device memory (32.95 GiB/94.62 GiB used) and -72 KiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:746] Loading model weights took in total 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.12 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:52:56 hpu_model_runner.py:746] Loading model weights took in total 32.95 GiB of device memory (32.95 GiB/94.62 GiB used) and 4.119 GiB of host memory (127.2 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:53:56 hpu_worker.py:240] Model profiling run took 3.439 GiB of device memory (36.39 GiB/94.62 GiB used) and 4.119 GiB of host memory (131.3 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:53:56 hpu_worker.py:264] Free device memory: 58.23 GiB, 52.41 GiB usable (gpu_memory_utilization=0.9), 5.241 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 47.17 GiB reserved for KV cache
INFO 12-09 23:53:56 hpu_worker.py:240] Model profiling run took 3.439 GiB of device memory (36.39 GiB/94.62 GiB used) and 3.274 GiB of host memory (130.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:53:56 hpu_worker.py:240] Model profiling run took 3.439 GiB of device memory (36.39 GiB/94.62 GiB used) and 3.26 GiB of host memory (130.5 GiB/1007 GiB used)
INFO 12-09 23:53:56 hpu_worker.py:264] Free device memory: 58.23 GiB, 52.41 GiB usable (gpu_memory_utilization=0.9), 5.241 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 47.17 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:53:56 hpu_worker.py:264] Free device memory: 58.23 GiB, 52.41 GiB usable (gpu_memory_utilization=0.9), 5.241 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 47.17 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:53:56 hpu_worker.py:240] Model profiling run took 3.439 GiB of device memory (36.39 GiB/94.62 GiB used) and 3.26 GiB of host memory (130.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:53:56 hpu_worker.py:264] Free device memory: 58.23 GiB, 52.41 GiB usable (gpu_memory_utilization=0.9), 5.241 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 47.17 GiB reserved for KV cache
INFO 12-09 23:53:56 distributed_gpu_executor.py:57] # GPU blocks: 4830, # CPU blocks: 409
INFO 12-09 23:53:56 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.72x
INFO 12-09 23:53:58 hpu_worker.py:297] Initializing cache engine took 47.17 GiB of device memory (83.56 GiB/94.62 GiB used) and 16.76 GiB of host memory (147.2 GiB/1007 GiB used)
INFO 12-09 23:53:58 hpu_model_runner.py:1617] Skipping warmup...
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:53:58 hpu_worker.py:297] Initializing cache engine took 47.17 GiB of device memory (83.56 GiB/94.62 GiB used) and 17.02 GiB of host memory (147.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=357)[0;0m INFO 12-09 23:53:58 hpu_model_runner.py:1617] Skipping warmup...
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:53:59 hpu_worker.py:297] Initializing cache engine took 47.17 GiB of device memory (83.56 GiB/94.62 GiB used) and 17.15 GiB of host memory (147.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=359)[0;0m INFO 12-09 23:53:59 hpu_model_runner.py:1617] Skipping warmup...
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:53:59 hpu_worker.py:297] Initializing cache engine took 47.17 GiB of device memory (83.56 GiB/94.62 GiB used) and 17.15 GiB of host memory (147.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=358)[0;0m INFO 12-09 23:53:59 hpu_model_runner.py:1617] Skipping warmup...
INFO 12-09 23:53:59 llm_engine.py:495] init engine (profile, create kv cache, warmup model) took 62.70 seconds
INFO 12-09 23:53:59 api_server.py:252] vLLM to use /tmp/tmpsrzzvs8g as PROMETHEUS_MULTIPROC_DIR
INFO 12-09 23:53:59 api_server.py:560] Using supplied chat template:
INFO 12-09 23:53:59 api_server.py:560] None
INFO 12-09 23:53:59 launcher.py:19] Available routes are:
INFO 12-09 23:53:59 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD
INFO 12-09 23:53:59 launcher.py:27] Route: /docs, Methods: GET, HEAD
INFO 12-09 23:53:59 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 12-09 23:53:59 launcher.py:27] Route: /redoc, Methods: GET, HEAD
INFO 12-09 23:53:59 launcher.py:27] Route: /health, Methods: GET
INFO 12-09 23:53:59 launcher.py:27] Route: /tokenize, Methods: POST
INFO 12-09 23:53:59 launcher.py:27] Route: /detokenize, Methods: POST
INFO 12-09 23:53:59 launcher.py:27] Route: /v1/models, Methods: GET
INFO 12-09 23:53:59 launcher.py:27] Route: /version, Methods: GET
INFO 12-09 23:53:59 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 12-09 23:53:59 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 12-09 23:53:59 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO 12-09 23:53:59 launcher.py:27] Route: /v1/score, Methods: POST
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:80 (Press CTRL+C to quit)
